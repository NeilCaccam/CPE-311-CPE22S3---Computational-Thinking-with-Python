{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "17OHcmQdQqmfVv18_gMkeM0Bk8XPAwd6N",
      "authorship_tag": "ABX9TyMPnVs1K0k2bwGRHY3SeypO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NeilCaccam/CPE-311-CPE22S3---Computational-Thinking-with-Python/blob/main/SW%207.2/Seatwork_7_2_Programming_Exercise_Data_Wrangling_with_Pandas_Part_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise Part 4:"
      ],
      "metadata": {
        "id": "q1XCsK9eG298"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Using the meteorite data from the Meteorite_Landings.csv file, create a pivot table that shows both the number of meteorites and the 95th percentile of meteorite mass for those that were found versus observed falling per year from 2005 through 2009 (inclusive). Hint: Be sure to convert the year column to a number as we did in the previous exercise."
      ],
      "metadata": {
        "id": "RfkrUn_yG7wF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40ZQwGZfETrp",
        "outputId": "30cee6c0-9bda-4aaf-bc2c-a6bd85fe4f7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Fell_<lambda_0>  Found_<lambda_0>  Fell_count  Found_count\n",
            "year                                                              \n",
            "2005.0              NaN           4500.00         NaN        874.0\n",
            "2006.0          25008.0           1600.50         5.0       2450.0\n",
            "2007.0          89675.0           1126.90         8.0       1181.0\n",
            "2008.0         106000.0           2274.80         9.0        948.0\n",
            "2009.0           8333.4           1397.25         5.0       1492.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1802443597.py:8: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df['year'] = pd.to_datetime(df['year'], errors='coerce').dt.year\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/DATA/Meteorite_Landings.csv')\n",
        "\n",
        "df['year'] = pd.to_datetime(df['year'], errors='coerce').dt.year\n",
        "\n",
        "# Filter for years 2005-2009\n",
        "df_filtered = df[(df['year'] >= 2005) & (df['year'] <= 2009)].copy()\n",
        "\n",
        "# Filter out rows with missing mass\n",
        "df_filtered = df_filtered[df_filtered['mass (g)'].notna()]\n",
        "\n",
        "pivot_table = df_filtered.pivot_table(\n",
        "    values='mass (g)',\n",
        "    index='year',\n",
        "    columns='fall',\n",
        "    aggfunc={\n",
        "        'mass (g)': ['count', lambda x: np.percentile(x, 95)]\n",
        "    }\n",
        ")\n",
        "\n",
        "pivot_table.columns = [f'{col[1]}_{col[0]}' for col in pivot_table.columns]\n",
        "\n",
        "print(pivot_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Using the meteorite data from the Meteorite_Landings.csv file, compare summary statistics of the mass column for the meteorites that were found versus observed falling."
      ],
      "metadata": {
        "id": "WMbeMfRsHCc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean = df[df['mass (g)'].notna()].copy()\n",
        "\n",
        "summary_stats = df_clean.groupby('fall')['mass (g)'].describe()\n",
        "\n",
        "summary_stats['cv_%'] = (summary_stats['std'] / summary_stats['mean']) * 100\n",
        "\n",
        "print(summary_stats[['count', 'mean', 'std', 'cv_%', 'min', '25%', '50%', '75%', 'max']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyHPodjYHI8X",
        "outputId": "d4e184db-6964-4212-ac20-e2f00e08cf61"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          count      mean        std     cv_%  min    25%      50%       75%  \\\n",
            "fall                                                                           \n",
            "Fell   1,075.00 47,070.72 717,067.13 1,523.38 0.10 686.00 2,800.00 10,450.00   \n",
            "Found 44,510.00 12,461.92 571,105.75 4,582.81 0.00   6.94    30.50    178.00   \n",
            "\n",
            "                max  \n",
            "fall                 \n",
            "Fell  23,000,000.00  \n",
            "Found 60,000,000.00  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise Part 5:"
      ],
      "metadata": {
        "id": "1g0Lv7xOI4fO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Using the taxi trip data in the 2019_Yellow_Taxi_Trip_Data.csv file, resample the data to an hourly frequency based on the dropoff time. Calculate the total trip_distance, fare_amount, tolls_amount, and tip_amount, then find the 5 hours with the most tips."
      ],
      "metadata": {
        "id": "oR-LFeB-I7Bg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/DATA/2019_Yellow_Taxi_Trip_Data.csv')\n",
        "\n",
        "df['tpep_dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'])\n",
        "df = df.set_index('tpep_dropoff_datetime')\n",
        "\n",
        "hourly = df.resample('H').agg({\n",
        "    'trip_distance': 'sum',\n",
        "    'fare_amount': 'sum',\n",
        "    'tolls_amount': 'sum',\n",
        "    'tip_amount': 'sum'\n",
        "})\n",
        "\n",
        "top_5 = hourly.nlargest(5, 'tip_amount')\n",
        "\n",
        "print(top_5[['tip_amount']])\n",
        "\n",
        "print(\"\\nDetailed view:\")\n",
        "print(top_5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYLsNSPoJAH0",
        "outputId": "5192c31f-6cf5-4961-b070-980746028026"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                       tip_amount\n",
            "tpep_dropoff_datetime            \n",
            "2019-10-23 16:00:00     12,228.64\n",
            "2019-10-23 17:00:00     12,044.03\n",
            "2019-10-23 18:00:00      1,907.64\n",
            "2019-10-23 15:00:00         51.75\n",
            "2019-10-23 19:00:00         25.74\n",
            "\n",
            "Detailed view:\n",
            "                       trip_distance  fare_amount  tolls_amount  tip_amount\n",
            "tpep_dropoff_datetime                                                      \n",
            "2019-10-23 16:00:00        10,676.95    67,797.76        699.04   12,228.64\n",
            "2019-10-23 17:00:00        16,052.83    70,131.91      4,044.04   12,044.03\n",
            "2019-10-23 18:00:00         3,104.56    11,565.56      1,454.67    1,907.64\n",
            "2019-10-23 15:00:00            14.34       213.50          0.00       51.75\n",
            "2019-10-23 19:00:00            98.59       268.00         24.48       25.74\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3299275882.py:12: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
            "  hourly = df.resample('H').agg({\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {
        "id": "e_maU3PMJ8TQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this exercise, I learned how to apply advanced pandas techniques to extract meaningful insights from real-world datasets. By working with the meteorite data, I practiced creating pivot tables to analyze the count and 95th percentile mass of meteorites based on whether they were found or observed falling, specifically filtering for the years 2005 through 2009. This revealed clear trends in the distribution and size of meteorites based on their recovery method. For the taxi trip data, I developed skills in time series analysis by resampling the data to an hourly frequency, which allowed me to aggregate trip totals and successfully identify the 5 hours with the highest tip amounts."
      ],
      "metadata": {
        "id": "u7O-DY7XJ-1p"
      }
    }
  ]
}